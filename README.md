# Image-Captioning-Text-to-Speech-System
üìå Image Captioning & Text-to-Speech (TTS) System
An AI-based accessibility tool for visually impaired users
This project transforms images and short video clips into descriptive captions and then converts those captions into speech audio, helping visually impaired users understand their surroundings.
Built using BLIP (Bootstrapping Language-Image Pretraining) for captioning and gTTS for audio synthesis.

üöÄ Project Overview
‚úÖ Image Captioning
Uses Salesforce/BLIP-image-captioning-base
Generates accurate captions for:
Single images
Multiple images
Extracted frames from short videos

‚úÖ Video Support
Processes 2‚Äì5 second MP4 videos
Extracts frames using OpenCV
Generates caption per frame ‚Üí creates a visual narrative

‚úÖ Audio Generation

Caption text ‚Üí spoken audio using gTTS
Automatically saves, plays, and deletes MP3 files
Includes sequential playback with delay for clarity

üåç Supported Languages
| Code   | Language          |
|--------|-------------------|
| en     | English           |
| es     | Spanish           |
| fr     | French            |
| de     | German            |
| it     | Italian           |
| pt     | Portuguese        |
| zh-CN  | Chinese (Mandarin)|
| hi     | Hindi             |
| ru     | Russian           |
| ar     | Arabic            |


üéØ Goal:
To provide real-time scene understanding to visually impaired individuals through audio descriptions.


üõ†Ô∏è Technologies & Libraries Used
| Purpose          | Library               |
| ---------------- | --------------------- |
| Image captioning | `transformers`        |
| Deep learning    | `torch`               |
| Image processing | `Pillow`              |
| Text-to-speech   | `gTTS`                |
| Audio playback   | `IPython.display`     |
| Video processing | `opencv-python (cv2)` |
| Utilities        | `os`, `time`          |


üß∞ How It Works
1Ô∏è‚É£ Load BLIP Model
Loads processor and model from HuggingFace
Converts media to RGB
Tokenizes input

2Ô∏è‚É£ Generate Caption
Model predicts caption using vision-language encoding

3Ô∏è‚É£ Convert Caption ‚Üí Speech
gTTS converts caption into MP3 audio
Audio is played using notebook display elements

4Ô∏è‚É£ Video Support
Extract frames using cv2.VideoCapture
Caption each frame
Generate a sequential speech narration

üîÆ Future Enhancements
Real-time webcam captioning
Mobile app deployment (TFLite)
Offline TTS/ML model
Improved UI



